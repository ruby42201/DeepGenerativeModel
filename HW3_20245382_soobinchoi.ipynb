{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in ./dgm_venv/lib/python3.9/site-packages (1.26.4)\n",
            "Requirement already satisfied: wheel in ./dgm_venv/lib/python3.9/site-packages (0.43.0)\n",
            "Requirement already satisfied: torch in ./dgm_venv/lib/python3.9/site-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in ./dgm_venv/lib/python3.9/site-packages (0.18.0)\n",
            "Requirement already satisfied: matplotlib in ./dgm_venv/lib/python3.9/site-packages (3.8.4)\n",
            "Requirement already satisfied: scipy in ./dgm_venv/lib/python3.9/site-packages (1.13.0)\n",
            "Requirement already satisfied: opencv-python in ./dgm_venv/lib/python3.9/site-packages (4.9.0.80)\n",
            "Requirement already satisfied: tqdm in ./dgm_venv/lib/python3.9/site-packages (4.66.4)\n",
            "Requirement already satisfied: networkx in ./dgm_venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: fsspec in ./dgm_venv/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: sympy in ./dgm_venv/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./dgm_venv/lib/python3.9/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: filelock in ./dgm_venv/lib/python3.9/site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: jinja2 in ./dgm_venv/lib/python3.9/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./dgm_venv/lib/python3.9/site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./dgm_venv/lib/python3.9/site-packages (from matplotlib) (6.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./dgm_venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.18.1)\n",
            "Requirement already satisfied: six>=1.5 in ./dgm_venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./dgm_venv/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./dgm_venv/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the '/Users/sov/Documents/Deep Generative Model/dgm_venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: pip in ./dgm_venv/lib/python3.9/site-packages (21.2.4)\n",
            "Collecting pip\n",
            "  Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.2.4\n",
            "    Uninstalling pip-21.2.4:\n",
            "      Successfully uninstalled pip-21.2.4\n",
            "Successfully installed pip-24.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy wheel torch torchvision matplotlib scipy opencv-python tqdm\n",
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtqwhK9q9D-h"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "## Overview\n",
        "This semester, all homeworks will be conducted through Google Colab notebooks. All code for the homework assignment will be written and run in this notebook. Running in Colab will automatically provide a GPU, but you may also run this notebook locally by following [these instructions](https://research.google.com/colaboratory/local-runtimes.html) if you wish to use your own GPU.\n",
        "\n",
        "You will save images in the notebooks to use and fill out a given LaTeX template which will be submitted to Gradescope, along with your notebook code.\n",
        "\n",
        "## Using Colab\n",
        "On the left-hand side, you can click the different icons to see a Table of Contents of the assignment, as well as local files accessible through the notebook.\n",
        "\n",
        "Make sure to go to **Runtime -> Change runtime type** and select **GPU** as the hardware accelerator. This allows you to use a GPU. Run the cells below to get started on the assignment. Note that a session is open for a maximum of 12 hours, and using too much GPU compute may result in restricted access for a short period of time. Please start the homework early so you have ample time to work.\n",
        "\n",
        "**If you loaded this notebook from clicking \"Open in Colab\" from github, you will need to save it to your own Google Drive to keep your work.**\n",
        "\n",
        "## General Tips\n",
        "In each homework problem, you will implement a GAN model and run it on a dataset. We provide the expected outputs to help as a sanity check.\n",
        "\n",
        "Feel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n",
        "\n",
        "After you complete the assignment, download all of the image outputted in the results/ folder and upload them to the figure folder in the given latex template.\n",
        "\n",
        "Run the cells below to download and load up the starter code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sfd-DkFA9AWI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deepul'...\n",
            "remote: Enumerating objects: 254, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 254 (delta 46), reused 32 (delta 32), pack-reused 182\u001b[K\n",
            "Receiving objects: 100% (254/254), 94.70 MiB | 13.72 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "Processing ./deepul\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: deepul\n",
            "  Building wheel for deepul (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for deepul: filename=deepul-0.1.0-py3-none-any.whl size=22778 sha256=a8efe91a79ceed15cb3113248b8700ae0353f20e2f387de785038b96091b18d8\n",
            "  Stored in directory: /private/var/folders/t9/gfxwh1615kd429xj7ws0q6840000gn/T/pip-ephem-wheel-cache-05suhn1l/wheels/51/6e/35/e00075dbebf5d4241237bc23fd04cf7aa43694b099848d560c\n",
            "Successfully built deepul\n",
            "Installing collected packages: deepul\n",
            "  Attempting uninstall: deepul\n",
            "    Found existing installation: deepul 0.1.0\n",
            "    Uninstalling deepul-0.1.0:\n",
            "      Successfully uninstalled deepul-0.1.0\n",
            "Successfully installed deepul-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!if [ -d deepul ]; then rm -Rf deepul; fi\n",
        "!git clone https://github.com/rll/deepul.git\n",
        "!pip install ./deepul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QSb6D-qQ9W7L"
      },
      "outputs": [],
      "source": [
        "from deepul.hw3_helper import *\n",
        "import deepul.pytorch_util as ptu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HKw4PzHd6bQs"
      },
      "outputs": [],
      "source": [
        "ptu.set_gpu_mode(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYa4i0uxspaA"
      },
      "source": [
        "# Question 1: Warmup [20pt]\n",
        "\n",
        "In this question, we will train different variants of GANs on an easy 1D dataset.\n",
        "\n",
        "Execute the cell below to visualize our datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fb5MioT8SZzN"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_q1_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "osR7PW_NLRu-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from scipy.stats import norm\n",
        "from tqdm import trange, tqdm_notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSOkSmHSL2c3"
      },
      "source": [
        "## Part 1(a)\n",
        "In this part, we'll train our generator and discriminator via the original minimax GAN objective:\n",
        "<insert GAN Objective here>\n",
        "$$min_{G} max_{D} \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log (1-D(G(z)))]$$\n",
        "\n",
        "Use an MLP for both your generator and your discriminator, and train until the generated distribution resembles the target distribution.\n",
        "* 3 layers\n",
        "* 128 hidden dim\n",
        "* LeakyReLU nonlinearities with negative_slope=0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKl7kyUPwPSJ"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.Linear(1, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input = 2\n",
        "        self.dis = nn.Sequential(\n",
        "            nn.Linear(self.input, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.dis(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\") #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "opim_g = torch.optim.Adam(generator.parameters(), lr = 1e-3)\n",
        "opim_d = torch.optim.Adam(discriminator.parameters(), lr = 1e-3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    real_label = torch.ones(b_size, 1).to(device)\n",
        "    fake_label = torch.zeros(b_size, 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output_real = discriminator(data_real)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "    loss_real.backward()\n",
        "    loss_fake.backward()\n",
        "    optimizer.step()\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    real_label = torch.ones(b_size, 1).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_together():\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_g = 0.0\n",
        "        loss_d = 0.0\n",
        "        for idx, data in tqdm(enumerate(train_loader), total = int(len(train_dataset)/train_loader.batch_size)):\n",
        "            value, _ = data\n",
        "            value = value.to(device)\n",
        "            b_size = len(value)\n",
        "            for step in range(k):\n",
        "                data_fake = generator(torch.randn(b_size, 1).to(device)).detach()\n",
        "                data_real = value\n",
        "                loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "            data_fake = generator(torch.randn(b_size, 1).to(device))\n",
        "            loss_g += train_generator(optim_g, data_fake)\n",
        "        epoch_loss_g = loss_g / idx\n",
        "        epoch_loss_d = loss_d / idx\n",
        "        losses_g.append(epoch_loss_g)\n",
        "        losses_d.append(epoch_loss_d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfyB3DFgKfA5"
      },
      "outputs": [],
      "source": [
        "def q1_a(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (20000, 1) numpy array of floats in [-1, 1]\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of discriminator losses evaluated every minibatch\n",
        "    - a numpy array of size (5000,) of samples drawn from your model at epoch #1\n",
        "    - a numpy array of size (1000,) linearly spaced from [-1, 1]; hint: np.linspace\n",
        "    - a numpy array of size (1000,), corresponding to the discriminator output (after sigmoid)\n",
        "        at each location in the previous array at epoch #1\n",
        "\n",
        "    - a numpy array of size (5000,) of samples drawn from your model at the end of training\n",
        "    - a numpy array of size (1000,) linearly spaced from [-1, 1]; hint: np.linspace\n",
        "    - a numpy array of size (1000,), corresponding to the discriminator output (after sigmoid)\n",
        "        at each location in the previous array at the end of training\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLrFM7TQwZkC"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icqjV8bEwfst"
      },
      "outputs": [],
      "source": [
        "q1_save_results('a', q1_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x4hfHRbZrDT"
      },
      "source": [
        "## Part 1(b)\n",
        "Here, we'll use the non-saturating formulation of the GAN objective. Now, we have two separate objectives:\n",
        "$$L^{(D)} = \\mathbb{E}_{x \\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log (1-D(G(z)))]$$\n",
        "$$L^{(G)} = - \\mathbb{E}_{z \\sim p(z)} \\log(D(G(z)))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uun1MzfwpuC"
      },
      "source": [
        " ### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJeEgn6zZst0"
      },
      "outputs": [],
      "source": [
        "def q1_b(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (20000, 1) numpy array of floats in [-1, 1]\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of discriminator losses evaluated every minibatch\n",
        "    - a numpy array of size (5000,) of samples drawn from your model at epoch #1\n",
        "    - a numpy array of size (100,) linearly spaced from [-1, 1]; hint: np.linspace\n",
        "    - a numpy array of size (100,), corresponding to the discriminator output (after sigmoid)\n",
        "        at each location in the previous array at epoch #1\n",
        "\n",
        "    - a numpy array of size (5000,) of samples drawn from your model at the end of training\n",
        "    - a numpy array of size (1000,) linearly spaced from [-1, 1]; hint: np.linspace\n",
        "    - a numpy array of size (1000,), corresponding to the discriminator output (after sigmoid)\n",
        "        at each location in the previous array at the end of training\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0VSrZWzwrzT"
      },
      "source": [
        " ### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfv-DeVKwtXl"
      },
      "outputs": [],
      "source": [
        "q1_save_results('b', q1_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBESzChmEfcF"
      },
      "source": [
        "# Question 2: GANs on CIFAR-10 [35pt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLRdpUxy5jc0"
      },
      "source": [
        "In this exercise, you will train a GAN on CIFAR-10. Execute the cell below to visualize the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WbafudL5mnz"
      },
      "outputs": [],
      "source": [
        "visualize_q2_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIYRnVafEkcd"
      },
      "source": [
        " We'll use the CIFAR-10 architecture from the [SN-GAN paper](https://arxiv.org/pdf/1802.05957.pdf) (see page 17), with $z \\in \\mathbb R ^{128}$, with $z \\sim \\mathcal N (0, I_{128})$. Instead of upsampling via transposed convolutions and downsampling via pooling or striding, we'll use these DepthToSpace and SpaceToDepth methods for changing the spatial configuration of our hidden states.\n",
        "\n",
        "```\n",
        "class DepthToSpace(nn.Module):\n",
        "    def __init__(self, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.block_size_sq = block_size * block_size\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.permute(0, 2, 3, 1)\n",
        "        (batch_size, d_height, d_width, d_depth) = output.size()\n",
        "        s_depth = int(d_depth / self.block_size_sq)\n",
        "        s_width = int(d_width * self.block_size)\n",
        "        s_height = int(d_height * self.block_size)\n",
        "        t_1 = output.reshape(batch_size, d_height, d_width, self.block_size_sq, s_depth)\n",
        "        spl = t_1.split(self.block_size, 3)\n",
        "        stack = [t_t.reshape(batch_size, d_height, s_width, s_depth) for t_t in spl]\n",
        "        output = torch.stack(stack, 0).transpose(0, 1).permute(0, 2, 1, 3, 4).reshape(batch_size, s_height, s_width,\n",
        "                                                                                      s_depth)\n",
        "        output = output.permute(0, 3, 1, 2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SpaceToDepth(nn.Module):\n",
        "    def __init__(self, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.block_size_sq = block_size * block_size\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.permute(0, 2, 3, 1)\n",
        "        (batch_size, s_height, s_width, s_depth) = output.size()\n",
        "        d_depth = s_depth * self.block_size_sq\n",
        "        d_width = int(s_width / self.block_size)\n",
        "        d_height = int(s_height / self.block_size)\n",
        "        t_1 = output.split(self.block_size, 2)\n",
        "        stack = [t_t.reshape(batch_size, d_height, d_depth) for t_t in t_1]\n",
        "        output = torch.stack(stack, 1)\n",
        "        output = output.permute(0, 2, 1, 3)\n",
        "        output = output.permute(0, 3, 1, 2)\n",
        "        return output\n",
        "\n",
        "# Spatial Upsampling with Nearest Neighbors\n",
        "Upsample_Conv2d(in_dim, out_dim, kernel_size=(3, 3), stride=1, padding=1):\n",
        "    x = torch.cat([x, x, x, x], dim=1)\n",
        "    DepthToSpace(block_size=2)\n",
        "    Conv2d(in_dim, out_dim, kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "\n",
        "# Spatial Downsampling with Spatial Mean Pooling\n",
        "Downsample_Conv2d(in_dim, out_dim, kernel_size=(3, 3), stride=1, padding=1):\n",
        "        SpaceToDepth(2)\n",
        "        torch.sum(x.chunk(4, dim=1)) / 4.0\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=bias)\n",
        "```\n",
        "\n",
        "Here's pseudocode for how we'll implement a ResBlockUp, used in the generator:\n",
        "\n",
        "```\n",
        "ResnetBlockUp(x, in_dim, kernel_size=(3, 3), n_filters=256):\n",
        "    _x = x\n",
        "    _x = nn.BatchNorm2d(in_dim)(_x)\n",
        "    _x = nn.ReLU()(_x)\n",
        "    _x = nn.Conv2d(in_dim, n_filters, kernel_size, padding=1)(_x)\n",
        "    _x = nn.BatchNorm2d(n_filters)(_x)\n",
        "    _x = nn.ReLU()(_x)\n",
        "    residual = Upsample_Conv2d(n_filters, n_filters, kernel_size, padding=1)(_x)\n",
        "    shortcut = Upsample_Conv2d(in_dim, n_filters, kernel_size=(1, 1), padding=0)(x)\n",
        "    return residual + shortcut\n",
        "```\n",
        "The ResBlockDown module is similar, except it uses Downsample_Conv2d and omits the BatchNorm.\n",
        "\n",
        "Finally, here's the architecture for the generator:\n",
        "```\n",
        "def Generator(*, n_samples=1024, n_filters=128):\n",
        "    z = Normal(0, 1)([n_samples, 128])\n",
        "    nn.Linear(128, 4*4*256)\n",
        "    reshape output of linear layer\n",
        "    ResnetBlockUp(in_dim=256, n_filters=n_filters),\n",
        "    ResnetBlockUp(in_dim=n_filters, n_filters=n_filters),\n",
        "    ResnetBlockUp(in_dim=n_filters, n_filters=n_filters),\n",
        "    nn.BatchNorm2d(n_filters),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(n_filters, 3, kernel_size=(3, 3), padding=1),\n",
        "    nn.Tanh()\n",
        "```\n",
        "The discriminator (no BatchNorm!).\n",
        "```\n",
        "def Discriminator(*):\n",
        "    def __init__(self, n_filters=128):\n",
        "        ResnetBlockDown(3, n_filters=n_filters),\n",
        "        ResnetBlockDown(128, n_filters=n_filters),\n",
        "        ResBlock(n_filters, n_filters=n_filters),\n",
        "        ResBlock(n_filters, n_filters=n_filters),\n",
        "        nn.ReLU()\n",
        "        global sum pooling\n",
        "        nn.Linear(128, 1)\n",
        "```\n",
        "\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "We'll implement [WGAN-GP](https://arxiv.org/abs/1704.00028), which uses a gradient penalty to regularize the discriminator. Use the Adam optimizer with $\\alpha = 2e-4$, $\\beta_1 = 0$, $\\beta_2 = 0.9$, $\\lambda = 10$, $n_{critic} = 5$. Use a batch size of 256 and n_filters=128 within the ResBlocks. Train for at least 25000 gradient steps, with the learning rate linearly annealed to 0 over training.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "1. Inception score (CIFAR-10 version) of the final model. We provide a utility that will automatically do this for you.\n",
        "2. Fr√©chet inception distance (bonus, 5pts)\n",
        "3. Discriminator loss across training\n",
        "4. 100 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlY4YYyedBlR"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zze30tQO7IjK"
      },
      "source": [
        " ### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8UX3mGtdB1A"
      },
      "outputs": [],
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of WGAN critic train losses evaluated every minibatch\n",
        "    - a (1000, 32, 32, 3) numpy array of samples from your model in [0, 1].\n",
        "        The first 100 will be displayed, and the rest will be used to calculate the Inception score.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\"\n",
        "\n",
        "    return losses, samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydRmPpLdEar"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRgINbsJdbPH"
      },
      "outputs": [],
      "source": [
        "q2_save_results(q2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3DUzaI-Cw3L"
      },
      "source": [
        "# Question 3 : Quantization with GANS [40pt]\n",
        "In this question, you'll train a variants of the VQGAN model to learn a mapping between image and a discrete codebook. We will do this on the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsGq3q-0Cw3L"
      },
      "source": [
        "## Part a: Vanilla VQGAN [25]\n",
        "[VQGAN](https://github.com/CompVis/taming-transformers) extends the ideas from VQVAE by adding an additional GAN loss. Review HW 2 for more details on VQVAE, and may use your code from HW 2 (or HW2 solutions) to help with this question. Conceptually, the training procedure is the same as VQVAE, but with the addition of a discriminator network and its training.\n",
        "\n",
        "Following the original VQGAN paper, we will use a patch-based discriminator. Split the image into 8x8 patches, which will be then fed into the discriminator. For the discrimiator you can use an architecture similar to the one used in the previous questions.\n",
        "```\n",
        "def Discriminator(*, n_filters=128):\n",
        "    split to 8x8 patches\n",
        "    ResnetBlockDown(3, n_filters=n_filters),\n",
        "    ResnetBlockDown(128, n_filters=n_filters),\n",
        "    ResBlock(n_filters, n_filters=n_filters),\n",
        "    ResBlock(n_filters, n_filters=n_filters),\n",
        "    nn.ReLU()\n",
        "    global sum pooling\n",
        "    nn.Linear(128, 1)\n",
        "```\n",
        "\n",
        "**Loss function**\n",
        "We will use the loss function of\n",
        "\n",
        "$L = L_{VQ} + 0.1 L_{GAN} + 0.5 L_{perceptual} + L_2$\n",
        "\n",
        "Where $L_{VQ}$ is the VQ objective, $L_{GAN}$ is the GAN loss, $L_{perceptual}$ is the perceptual loss.\n",
        "\n",
        "For pytorch users, we provide the LPIPS loss from the original taming transformers paper ([found here](https://github.com/CompVis/taming-transformers/blob/master/taming/modules/losses/lpips.py)). An example of using the LPIPS loss is found below. If you are using a different framework, feel free to use any off the shelf code, just make sure to cite where you take it from.\n",
        "\n",
        "Instead of using the adaptive weight on the GAN loss, we will use a fixed weight of 0.1.\n",
        "* train for at least 15 epochs\n",
        "* use a codebook size of 1024\n",
        "* use Adam with betas=(0.5, 0.9) and 0 weight decay for both optimizers\n",
        "* use the non-saturating formulation of the GAN objective.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "1. Final L2 reconstruction loss\n",
        "2. Discriminator loss across training\n",
        "3. $L_{perceptual}$ loss across training\n",
        "4. Training l2 loss across training\n",
        "5. Validation l2 reconstruction loss across training, evaluated at the end of every epoch\n",
        "6. 100 reconstructions of the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXB33DYVCw3L"
      },
      "outputs": [],
      "source": [
        "# example usage of the LPIPS loss\n",
        "from deepul.hw3_utils.lpips import LPIPS\n",
        "loss = LPIPS()\n",
        "x = torch.zeros(4, 3, 32, 32)\n",
        "print(loss(x, x).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT_yyexjCw3M"
      },
      "outputs": [],
      "source": [
        "def q3a(train_data, val_data, reconstruct_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
        "    val_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
        "    reconstruct_data: An (100, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]. To be used for reconstruction\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of the discriminator train losses evaluated every minibatch\n",
        "    - None or a (# of training iterations,) numpy array of the perceptual train losses evaluated every minibatch\n",
        "    - a (# of training iterations,) numpy array of the l2 reconstruction evaluated every minibatch\n",
        "    - a (# of epochs + 1,) numpy array of l2 reconstruction loss evaluated once at initialization and after each epoch on the val_data\n",
        "    - a (100, 32, 32, 3) numpy array of reconstructions from your model in [0, 1] on the reconstruct_data.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\"\n",
        "\n",
        "    return discriminator_losses, l_pips_losses, l2_recon_train, l2_recon_test, reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOZmMQZaCw3M"
      },
      "outputs": [],
      "source": [
        "q3_save_results(q3a, \"a\") # with pips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZcfmgvwCw3M"
      },
      "source": [
        "## Part b: VIT-VQGAN [15]\n",
        "In this part, you will replace the encoder and decoder in the VQGAN with a Vision Transformer (ViT) following the [Improved VQGAN paper](https://arxiv.org/abs/2110.04627). You may use pre-built ViT modules and/or transformers libraries for the purposes of this exercise. Split images into 4x4 patches which should give you 8x8 patches for each image.\n",
        "\n",
        "**Discriminator**\n",
        "In ViT-VQGAN, the patch-based discriminator is replaced with a full StyleGAN discriminator. For the purposes of this exercise, you should use the same discrimiator as described in part a but without splitting into patches. In addition add [spectral normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html#torch-nn-utils-spectral-norm) to all the conv and linear layers and use LeakyReLU instead of ReLU, following [SN-GANs](https://arxiv.org/abs/1802.05957). This is to help with training stability.\n",
        "```\n",
        "def Discriminator(*, n_filters=128):\n",
        "    # all conv layers in the ResBlocks should have spectral normalization\n",
        "    ResnetBlockDown(3, n_filters=n_filters),\n",
        "    ResnetBlockDown(128, n_filters=n_filters),\n",
        "    ResBlock(n_filters, n_filters=n_filters),\n",
        "    ResBlock(n_filters, n_filters=n_filters),\n",
        "    nn.LeakyReLU()\n",
        "    global sum pooling\n",
        "    nn.Linear(128, 1)\n",
        "```\n",
        "* in addition add\n",
        "\n",
        "**Loss function**\n",
        "$L = L_{VQ} + 0.1 L_{GAN} + \\alpha L_{perceptual} + L_2 + 0.1 L_1$\n",
        "\n",
        "The ViT-VQGAN paper introduces a new loss term $L_{Logit\\_laplace}$ which is a logit laplace loss. For simplicity, we will use an L1 loss instead.\n",
        "\n",
        "If you implemented the perceptual loss in the previous part, you can use the same perceptual loss here, otherwise you may keep $\\alpha = 0$.\n",
        "\n",
        "\n",
        "You should be able to largely be able to reuse your implementations in the previous part for this problem\n",
        "The only 3 updates you need are:\n",
        "* use ViT for encoder and decoder of VQVAE, use 4 layers, 8 heads, and 256 for the embedding size\n",
        "* add L1 loss\n",
        "* change the discrimiator to look at the full image\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "1. Final L2 reconstruction loss\n",
        "2. Discriminator loss across training\n",
        "3. $L_{perceptual}$ loss across training\n",
        "4. Training l2 loss across training\n",
        "5. Validation l2 reconstruction loss across training, evaluated at the end of every epoch\n",
        "6. 100 reconstructions of the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYyv5aneCw3M"
      },
      "outputs": [],
      "source": [
        "def q3b(train_data, val_data, reconstruct_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
        "    val_data: An (n_train, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]\n",
        "    reconstruct_data: An (100, 3, 32, 32) numpy array of CIFAR-10 images with values in [0, 1]. To be used for reconstruction\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations,) numpy array of the discriminator train losses evaluated every minibatch\n",
        "    - None or a (# of training iterations,) numpy array of the perceptual train losses evaluated every minibatch\n",
        "    - a (# of training iterations,) numpy array of the l2 reconstruction evaluated every minibatch\n",
        "    - a (# of epochs + 1,) numpy array of l2 reconstruction loss evaluated once at initialization and after each epoch on the val_data\n",
        "    - a (100, 32, 32, 3) numpy array of reconstructions from your model in [0, 1] on the reconstruct_data.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" YOUR CODE HERE \"\"\"\n",
        "\n",
        "    return discriminator_losses, l_pips_losses, l2_recon_train, l2_recon_test, reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CBQ5EaPCw3M"
      },
      "outputs": [],
      "source": [
        "q3_save_results(q3b, \"b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f0jUFadqml"
      },
      "source": [
        "# Question 4 (Bonus): CycleGAN [20pt]\n",
        "In this question, you'll train a CycleGAN model to learn to translate between two different image domains, without any paired data. Execute the following cell to visualize our two datasets: MNIST and Colored MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3qFm_JfObPj"
      },
      "outputs": [],
      "source": [
        "visualize_cyclegan_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Za8w7ddOdh7"
      },
      "source": [
        "In [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf), the goal is to learn functions $F$ and $G$ that can transform images from $X \\rightarrow Y$ and vice-versa. This is an unconstrained problem, so we additionally enforce the *cycle-consistency* property, where we want\n",
        "$$x \\approx G(F(x))$$\n",
        "and  \n",
        "$$y \\approx F(G(x))$$\n",
        "This loss function encourages $F$ and $G$ to approximately invert each other. In addition to this cycle-consistency loss, we also have a standard GAN loss such that $F(x)$ and $G(y)$ look like real images from the other domain.\n",
        "\n",
        "Since this is a bonus question, we won't do much hand-holding. We recommend reading through the original paper to get a sense of what architectures and hyperparameters are useful. Note that our datasets are fairly simple, so you won't need excessively large models.\n",
        "\n",
        "**You will report the following deliverables**\n",
        "1. A set of images showing real MNIST digits, transformations of those images into Colored MNIST digits, and reconstructions back into the greyscale domain.\n",
        "2. A set of images showing real Colored MNIST digits, transformations of those images, and reconstructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8adRfm9vPnen"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B99F5FjbPqtl"
      },
      "outputs": [],
      "source": [
        "def q4(mnist_data, cmnist_data):\n",
        "    \"\"\"\n",
        "    mnist_data: An (60000, 1, 28, 28) numpy array of black and white images with values in [0, 1]\n",
        "    cmnist_data: An (60000, 3, 28, 28) numpy array of colored images with values in [0, 1]\n",
        "\n",
        "    Returns\n",
        "    - a (20, 28, 28, 1) numpy array of real MNIST digits, in [0, 1]\n",
        "    - a (20, 28, 28, 3) numpy array of translated Colored MNIST digits, in [0, 1]\n",
        "    - a (20, 28, 28, 1) numpy array of reconstructed MNIST digits, in [0, 1]\n",
        "\n",
        "    - a (20, 28, 28, 3) numpy array of real Colored MNIST digits, in [0, 1]\n",
        "    - a (20, 28, 28, 1) numpy array of translated MNIST digits, in [0, 1]\n",
        "    - a (20, 28, 28, 3) numpy array of reconstructed Colored MNIST digits, in [0, 1]\n",
        "    \"\"\"\n",
        "    \"\"\" YOUR CODE HERE \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIO0hzZ8PpPr"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hlvbDvUOxp6"
      },
      "outputs": [],
      "source": [
        "q4_save_results(q4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
